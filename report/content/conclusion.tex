In the COST paper the authors show it is important to compare a parallel solution to the fastest purpose-build single-threaded implementation. Here I have attempted to reproduce part of their experiments, comparing \textit{PageRank} and \textit{Label Propagation} on the GraphX platform to their custom implementation.

If we look at the runtime of said implementation \cref{tab:rust_pagerank} we might think reordering the graphs is useless since it sometimes takes more time then doing the calculation. However we need to realise that this is including reading from disk three times (two conversions + one actual pagerank execution) and writing twice. This could be optimized to reading only once or if more operations on the graphs in \textit{Hilbert} order are needed this cost could be spread out across those operations. Note that even with all this extra work doing all conversions saves us time for the largest of our graph: \textit{graph500-25}.

For the graphs that could successfully run with GraphX we have to conclude the COST metric seems infinite. I tried running with a higher core count as extrapolation suggests that GraphX might beat the single threaded solution at 512 cores however this would not run due to the very high memory requirement. We do see that the larger the dataset is the more it gains from parallelism. 

My GraphX Label Propagation implementation would not run on any number of cores within a reasonable time. It was tested with trivial graphs of just tens of vertices and found to be very slow but functional. I probably made a mistake in the code or in preparing the Spark cluster or the used version of GraphX might not ship with a (fast enough) working Label Propagation function. 

Comparing to the results found by the authors \cite{189908} our distributed system run slower. They found that only their largest graph, with 500 times more edges then the largest graph I tried, allowed GraphX to finish faster. The first experiment seems reproduced. The second however is not as Label Propagation was to slow to even finish. This does not mean the authors result do not stand it does however indicate something was wrong with the experiments as performed by me.

There are a number of ways my work here could be improved. The GraphX implantation could be sped up using a binary edgelist or even a webgraph compression\cite{webgraph} instead of the current ASCII encoded one. I have doubts on the reliability of the IO-time measurements, these are based on some assumptions about the inner workings of the authors implementation that might not be valid. And finally the graph order conversion should be integrated in the execution of pagerank and label propagation to prevent unnecessary reads and writes to disk.
