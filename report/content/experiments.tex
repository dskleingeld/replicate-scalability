Here we look closer at the \textit{PageRank} and \todo{other} experiment, first we detail the authors work before proposing an alternative approach.

\subsection{Original Design} \label{sec:hilbert}
To determine the \textit{COST} a best in class single threaded implementation is needed. The authors wrote their own implementation for both experiments in \textit{Rust}. Their implementation initially loaded the graphs edges in vertex order, loading all edges for one vertex before moving to the next. \textit{GraphLab} and \textit{GraphX} save on data exchange by partition the edges between workers without this requirement \cite{graphlab,graphx}. To remove this disadvantage they wrote a second implementation was tested where the edges are pre ordered using Hilbert order improving cache coherency. They report the run time for both vertex orders.

Both experiments where originally ran on two graphs, \textit{twitter\_rv} \cite{twitter} and \textit{uk-2007-05} \cite{uk2007}. For ease of replication I deviate and run on different graphs.

\subsection{Implementation} \label{sec:hilbert}
\subsubsection{PageRank}
The pagerank algorithm iteratively updates a rank for each vertex in a directed graph. Each iteration the rank is divided between a vertex neighbours. Then the vertex rank is updated to the sum of its neighbours with a dampening factor applied. It was the originally algorithm used by Google to rank a websites significance. For this experiment I use the authors \textit{PageRank} implementation for the single threaded pagerank. For \textit{GraphX} a slightly modified version of the pagerank example\footnote{https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark /examples/graphx/PageRankExample.scala} in scala is used. We remove all operations after the call to \textit{pageRank} and we replace that call to \textit{staticPageRank} with 20 iterations as the argument to match the authors rust implementation.

\subsubsection{Label Propagation}
In distributed computing label propagation is often used to find connected components. Each individual vertex gets a label then it is iteratively updated. Each update the label is set to the minimum of all the neighbours and itself. Eventually all components have the smallest label of that component. For the single threaded part of this experiment I use the authors code unmodified. For \textit{GraphX} there is no label propagation example there is however a label propagation function a member of the \textit{graph class}. From this function I wrote the basic implementation in \cref{lst:labelprop}. The part after line 22 was added to verify correctness versus the authors implementation.

\begin{lstlisting}[caption={Scala Label Propagation for spark},language=Scala,numbers=left,breaklines=true, label={lst:labelprop}]
import org.apache.spark.graphx.GraphLoader
import org.apache.spark.graphx.lib.LabelPropagation
import org.apache.spark.sql.SparkSession

object LabelProp {
  def main(args: Array[String]): Unit = {
    // Creates a SparkSession.
    val spark = SparkSession
      .builder
      .appName(s"${this.getClass.getSimpleName}")
      .getOrCreate()
    val sc = spark.sparkContext

    // $example on$
    // Load the edges as a graph
    val edgeListPath = args(0)
    println("edgeListPath: %s", edgeListPath)
    val graph = GraphLoader.edgeListFile(sc, edgeListPath)
    println("graph loaded")
    // Run LabelProp, second arg is max numb of iterations
    val labels = LabelPropagation.run(graph, 1)
    println("test")

    // count non root labels
    // scala tuple access: tuple_.n with n in {1...tuple_len)
    val non_roots = labels.vertices.filter(x => x._1 != x._2).count()
    println("non roots: ", non_roots)

    spark.stop()
  }
}
\end{lstlisting}

\subsection{Spark}
A standalone version of Spark was placed into a folder accessible to each compute node. Then using the, by Spark provided, submit script the master and driver are started. The script automatically sets up all the workers from a file of ips provided in the Spark config directory, called 'slaves'. Via the command line arguments of the spark submit script we can control:


Before deployment the needed data is moved to high speed ssd storage for the workers and driver. We vary the values determining the parallism only the memory is set to a fixed amount. The variables set (excluding which program to run and the master node url):

\begin{multicols}{2}
\begin{enumerate}
	\item number executors
	\item total cores used by the executors
	\item cores per executor
	\item cores for the driver
	\item memory per executor: 48GB
	\item memory for the driver: 48GB
\end{enumerate}
\end{multicols}

The number of executors is locked to the total number of cores in use, allow spark itself to determine the needed parallism. The number of cores is varyed from 2 to 256.

\subsection{Extension}
The exact configuration of the platforms or the specification on which either parallel or single-threaded implementations where run are unknown. This could be an issue as the cores on which the single threaded code is ran could be significantly slower or faster then the used cluster. Here I run the single-threaded code on one of the compute nodes of the cluster on which \textit{GraphX} is run. The full specification of the used system is reported in \cref{res}.

As described in \cref{sec:hilbert} the authors converted the vertex order before starting running one variant of their implementation. The conversion time is mentioned in the paper for the smaller of the graphs. For completion we include the conversion time for all the graphs. Further more it is unclear if measured times are averages or single data points, I try to improve upon this by running the experiments multiple times and reporting the standard deviation.
