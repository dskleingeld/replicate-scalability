Recent decades have seen the introduction of many very scalable systems. Most publications focus on this impressive scalability, that is the presented system performs far better as it was given more parallel compute ability. It is unknown how these systems compare to classical non distributed sequential implementations. Here I look at, and reproduce part of, the work of Frank McSherry, Michael Isard and Derek G. Murray \cite{189908}. They offer a new performance metric aimed to solve this and survey a number of data-parallel systems.

In their paper they look at the performance of graph processing systems compared to simple single-threaded implementations. The performance is expressed in the COST, the \textit{configuration that outperforms the best single-threaded implementation} where they wrote these single-threaded implementations. The authors warn their results are not perfect nor fair however some concern must be raised because of the outcomes. They show there are systems for which the COST is infinite, not even an infinite number of cores allowes such a system to beat the best single-threaded implementation.

In their paper are three experiments or \textit{benchmarks} ran on the following graph processing systems:

\begin{multicols}{2}
\begin{enumerate}
	\item GraphChi 
	\item Stratosphere
	\item X-Stream
	\item Spark
	\item Giraph
	\item GraphLab
	\item GraphX
\end{enumerate}
\end{multicols}

In the first experiment the runtime of \texttt{PageRank} is measured for two graphs the second experiment measures the runtime of finding connected components using \texttt{label propagation} on the same graphs. The authors then note that label propagation is used for finding graph connectivity due to its scalability while its not a good choice for the problem. Therefore they introduce a third experiment, benchmarking \texttt{Union-Find} with weighted union on the graphs. In the next section (\cref{exp}) we will look closer at the \texttt{PageRank} and \texttt{label propagation} experiments and see if their design can be improved. Then in \cref{res} we see the results of reproducing these experiments with the \texttt{GraphX} platform. Finally we analyse these results and see if we achieved the same as the authors.
